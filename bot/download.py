"""
Complete Download Module - ULTRA-FAST OPTIMIZED (Part 1)
"""
import os
import asyncio
import aiohttp
import aiofiles
import urllib.parse
from loguru import logger
import config

async def get_download_info(terabox_url: str, status_msg=None):
    """Get download information from WDZone API with compatible return format"""
    
    try:
        # Properly encode the URL
        encoded_url = urllib.parse.quote(terabox_url, safe='')
        api_url = f"https://wdzone-terabox-api.vercel.app/api?url={encoded_url}"
        
        logger.info(f"üîÑ API Request: {api_url[:100]}...")
        
        timeout = aiohttp.ClientTimeout(total=30)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9'
        }
        
        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(api_url) as response:
                logger.info(f"üì° API Response Status: {response.status}")
                
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"üìä API Response Keys: {list(data.keys())}")
                    
                    # Handle WDZone API format
                    status_key = None
                    extracted_key = None
                    
                    # Find status key (with or without emoji)
                    for key in data.keys():
                        if "Status" in key or "status" in key:
                            status_key = key
                        if "Extracted" in key or "extracted" in key:
                            extracted_key = key
                    
                    if status_key and data.get(status_key) == "Success" and extracted_key:
                        extracted_info = data[extracted_key]
                        
                        if isinstance(extracted_info, list) and len(extracted_info) > 0:
                            file_info = extracted_info[0]
                        else:
                            file_info = extracted_info
                        
                        # Extract file details (handle emoji keys)
                        file_name = None
                        file_size_str = None
                        download_url = None
                        
                        # Find title/name key
                        for key, value in file_info.items():
                            if "Title" in key or "title" in key or "name" in key:
                                file_name = value
                            elif "Size" in key or "size" in key:
                                file_size_str = value
                            elif "Direct" in key or "download" in key or "link" in key:
                                download_url = value
                        
                        # Default fallback if keys not found
                        if not file_name:
                            file_name = file_info.get("Title", file_info.get("title", "unknown_file"))
                        if not file_size_str:
                            file_size_str = file_info.get("Size", file_info.get("size", "0"))
                        if not download_url:
                            download_url = file_info.get("Direct Download Link", file_info.get("download_url"))
                        
                        # Handle size conversion for numeric value
                        try:
                            if isinstance(file_size_str, str):
                                import re
                                # Extract number from "30.56 MB" format
                                size_match = re.search(r'([\d.]+)', file_size_str)
                                if size_match:
                                    size_num = float(size_match.group(1))
                                    if "MB" in file_size_str.upper():
                                        file_size = int(size_num * 1024 * 1024)
                                    elif "GB" in file_size_str.upper():
                                        file_size = int(size_num * 1024 * 1024 * 1024)
                                    elif "KB" in file_size_str.upper():
                                        file_size = int(size_num * 1024)
                                    else:
                                        file_size = int(size_num)
                                else:
                                    file_size = 0
                            else:
                                file_size = int(file_size_str)
                        except:
                            file_size = 0
                        
                        if download_url and file_name:
                            size_mb = file_size / (1024 * 1024) if file_size else 0
                            logger.info(f"‚úÖ WDZone API Success - File: {file_name}, Size: {size_mb:.2f} MB")
                            logger.info(f"üîó Download URL: {download_url[:100]}...")
                            
                            # Return in the format handlers expect
                            return {
                                "success": True,
                                "filename": file_name,
                                "size": file_size_str,  # Keep original size string
                                "download_url": download_url,
                                "file_size": file_size  # Also provide numeric size
                            }
                    
                    logger.error(f"‚ùå API returned unexpected format: {str(data)[:500]}...")
                    return {
                        "success": False,
                        "error": "API response format not recognized"
                    }
                    
                else:
                    response_text = await response.text()
                    logger.error(f"‚ùå API request failed with status {response.status}: {response_text[:200]}...")
                    return {
                        "success": False,
                        "error": f"API request failed: {response.status}"
                    }
                    
    except Exception as e:
        logger.error(f"‚ùå API request error: {e}")
        return {
            "success": False,
            "error": str(e)
        }

def _sanitize_filename(filename: str) -> str:
    """Sanitize filename for safe storage"""
    import re
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    filename = re.sub(r'\s+', ' ', filename).strip()
    if len(filename) > 200:
        name, ext = os.path.splitext(filename)
        filename = name[:190] + ext
    return filename

async def download_chunk(url: str, start: int, end: int, chunk_id: int):
    """Download a specific chunk of the file in parallel"""
    try:
        headers = {
            'Range': f'bytes={start}-{end}',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': '*/*',
            'Connection': 'keep-alive'
        }
        
        timeout = aiohttp.ClientTimeout(total=300, sock_read=60)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, headers=headers) as response:
                if response.status in [206, 200, 416]:  # Partial content, full content, or range not satisfiable
                    data = await response.read()
                    logger.info(f"üì¶ Chunk {chunk_id} downloaded: {len(data)/(1024*1024):.1f} MB")
                    return chunk_id, data
                else:
                    logger.warning(f"‚ö†Ô∏è Chunk {chunk_id} failed with status {response.status}")
                    
    except Exception as e:
        logger.error(f"‚ùå Chunk {chunk_id} download failed: {e}")
    
    return chunk_id, None

async def download_parallel_chunks(download_url: str, file_path: str, total_size: int, status_msg):
    """PARALLEL DOWNLOAD: Download file in multiple chunks simultaneously"""
    try:
        # Calculate optimal chunk count (2-6 chunks based on file size)
        if total_size < 10 * 1024 * 1024:  # < 10MB
            chunk_count = 2
        elif total_size < 50 * 1024 * 1024:  # < 50MB
            chunk_count = 4
        else:  # >= 50MB
            chunk_count = 6
        
        chunk_size = total_size // chunk_count
        logger.info(f"üî• PARALLEL DOWNLOAD: {chunk_count} chunks of ~{chunk_size/(1024*1024):.1f} MB each")
        
        tasks = []
        for i in range(chunk_count):
            start = i * chunk_size
            end = start + chunk_size - 1 if i < chunk_count - 1 else total_size - 1
            
            task = download_chunk(download_url, start, end, i)
            tasks.append(task)
        
        # Download all chunks in parallel
        await status_msg.edit_text(f"üî• Parallel downloading {chunk_count} chunks...", parse_mode=None)
        chunks_data = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Sort chunks by ID and combine
        valid_chunks = []
        for result in chunks_data:
            if isinstance(result, tuple) and result[1] is not None:
                valid_chunks.append(result)
        
        if len(valid_chunks) == chunk_count:
            # Sort by chunk ID
            valid_chunks.sort(key=lambda x: x[0])
            
            # Write combined file
            with open(file_path, 'wb') as outfile:
                for chunk_id, chunk_data in valid_chunks:
                    outfile.write(chunk_data)
            
            if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                final_size = os.path.getsize(file_path) / (1024 * 1024)
                logger.info(f"‚úÖ PARALLEL download successful! Size: {final_size:.2f} MB")
                return file_path
        else:
            logger.warning(f"‚ö†Ô∏è Parallel download incomplete: {len(valid_chunks)}/{chunk_count} chunks")
            
    except Exception as e:
        logger.error(f"‚ùå Parallel download failed: {e}")
        
    return None
async def download_file(download_url: str, filename: str, status_msg):
    """ULTRA-FAST Download with advanced optimizations (TARGET: 2-4x SPEED BOOST)"""
    filename = _sanitize_filename(filename)
    file_path = os.path.join(config.DOWNLOAD_DIR, filename)
    os.makedirs(config.DOWNLOAD_DIR, exist_ok=True)
    
    logger.info(f"üöÄ Starting ULTRA-FAST download: {filename}")
    logger.info(f"üîó Download URL: {download_url[:100]}...")
    
    # Strategy 1: PARALLEL CHUNK DOWNLOAD (FASTEST - 3-4x speed boost)
    try:
        await status_msg.edit_text("üî• Checking parallel download support...", parse_mode=None)
        logger.info("üîÑ Attempting PARALLEL CHUNK download")
        
        # Test if server supports range requests
        timeout = aiohttp.ClientTimeout(total=30)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.head(download_url) as response:
                if response.status == 200:
                    accept_ranges = response.headers.get('Accept-Ranges', '').lower()
                    content_length = response.headers.get('Content-Length')
                    
                    if accept_ranges == 'bytes' and content_length:
                        total_size = int(content_length)
                        if total_size > 5 * 1024 * 1024:  # Only for files > 5MB
                            logger.info(f"üî• Server supports parallel download! Size: {total_size/(1024*1024):.1f} MB")
                            
                            result = await download_parallel_chunks(download_url, file_path, total_size, status_msg)
                            if result:
                                return result
                        else:
                            logger.info("üìù File too small for parallel download, using optimized single stream")
                    else:
                        logger.info("üìù Server doesn't support range requests, using optimized single stream")
                        
    except Exception as e:
        logger.warning(f"Parallel download check failed: {e}")
    
    # Strategy 2: ULTRA-FAST SINGLE STREAM (OPTIMIZED TCP - 2x speed boost)
    try:
        await status_msg.edit_text("üöÄ Ultra-fast single stream download...", parse_mode=None)
        logger.info("üîÑ Attempting ULTRA-FAST OPTIMIZED download")
        
        # Advanced TCP connector optimization for Koyeb
        connector = aiohttp.TCPConnector(
            limit=50,                   # Total connection pool size
            limit_per_host=10,          # Connections per host
            ttl_dns_cache=300,          # DNS cache TTL (5 minutes)
            use_dns_cache=True,         # Enable DNS caching
            keepalive_timeout=60,       # Keep-alive timeout
            enable_cleanup_closed=True, # Clean up closed connections
            force_close=False,          # Reuse connections
            tcp_nodelay=True           # Disable Nagle's algorithm for speed
        )
        
        timeout = aiohttp.ClientTimeout(
            total=300,                  # 5 minutes total
            sock_read=45,               # 45 seconds read timeout
            sock_connect=15             # 15 seconds connect timeout
        )
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Connection': 'keep-alive',
            'Accept-Encoding': 'gzip, deflate, br'
        }
        
        async with aiohttp.ClientSession(
            timeout=timeout, 
            headers=headers, 
            connector=connector
        ) as session:
            async with session.get(download_url) as response:
                logger.info(f"üì° Download Response Status: {response.status}")
                
                if response.status == 200:
                    content_length = response.headers.get('Content-Length')
                    if content_length:
                        total_size = int(content_length)
                        total_mb = total_size / (1024*1024)
                        logger.info(f"üìä File size: {total_mb:.2f} MB")
                    
                    async with aiofiles.open(file_path, 'wb') as file:
                        downloaded = 0
                        chunk_size = 1024 * 1024 * 3  # 3MB chunks for maximum speed!
                        start_time = asyncio.get_event_loop().time()
                        last_progress_time = start_time
                        
                        async for chunk in response.content.iter_chunked(chunk_size):
                            await file.write(chunk)
                            downloaded += len(chunk)
                            
                            # Update progress every 2 seconds or 5MB
                            current_time = asyncio.get_event_loop().time()
                            if (downloaded % (5 * 1024 * 1024) == 0) or (current_time - last_progress_time >= 2):
                                mb_downloaded = downloaded / (1024 * 1024)
                                elapsed_time = current_time - start_time
                                speed = downloaded / elapsed_time / (1024 * 1024) if elapsed_time > 0 else 0
                                logger.info(f"üöÄ ULTRA FAST: {mb_downloaded:.1f} MB @ {speed:.1f} MB/s")
                                try:
                                    await status_msg.edit_text(f"üöÄ Downloaded: {mb_downloaded:.1f} MB @ {speed:.1f} MB/s", parse_mode=None)
                                except:
                                    pass
                                last_progress_time = current_time
                    
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        final_size = os.path.getsize(file_path) / (1024 * 1024)
                        total_time = asyncio.get_event_loop().time() - start_time
                        avg_speed = final_size / total_time if total_time > 0 else 0
                        logger.info(f"‚úÖ ULTRA-FAST download successful! Size: {final_size:.2f} MB in {total_time:.1f}s @ {avg_speed:.1f} MB/s")
                        return file_path
                        
    except Exception as e:
        logger.warning(f"Ultra-fast download failed: {e}")
    
    # Strategy 3: HIGH-SPEED DOWNLOAD (Fallback - still 2x faster than original)
    try:
        await status_msg.edit_text("‚ö° High-speed downloading...", parse_mode=None)
        logger.info("üîÑ Attempting HIGH-SPEED download")
        
        timeout = aiohttp.ClientTimeout(total=600, sock_read=120)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': '*/*',
            'Connection': 'keep-alive'
        }
        
        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            async with session.get(download_url) as response:
                if response.status == 200:
                    async with aiofiles.open(file_path, 'wb') as file:
                        chunk_size = 1024 * 1024 * 2  # 2MB chunks
                        async for chunk in response.content.iter_chunked(chunk_size):
                            await file.write(chunk)
                    
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        logger.info("‚úÖ High-speed download successful!")
                        return file_path
                        
    except Exception as e:
        logger.warning(f"High-speed download failed: {e}")
    
    # Strategy 4: STANDARD FAST (Final fallback)
    try:
        await status_msg.edit_text("üì• Standard fast downloading...", parse_mode=None)
        logger.info("üîÑ Attempting standard fast download")
        
        timeout = aiohttp.ClientTimeout(total=900, sock_read=180)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(download_url) as response:
                if response.status == 200:
                    async with aiofiles.open(file_path, 'wb') as file:
                        chunk_size = 1024 * 1024  # 1MB chunks
                        async for chunk in response.content.iter_chunked(chunk_size):
                            await file.write(chunk)
                    
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        logger.info("‚úÖ Standard fast download successful!")
                        return file_path
                        
    except Exception as e:
        logger.error(f"Standard fast download failed: {e}")
    
    logger.error("‚ùå All download strategies failed")
    return None

# TeraboxDownloader class for backward compatibility
class TeraboxDownloader:
    """Compatibility class for existing handlers - ULTRA-FAST OPTIMIZED"""
    
    def __init__(self):
        pass
    
    async def get_session(self):
        """Dummy method for compatibility"""
        pass
    
    async def close_session(self):
        """Dummy method for compatibility"""
        pass
    
    async def get_download_info(self, terabox_url: str, status_msg=None, *args, **kwargs):
        """Get download info - flexible parameter handling"""
        return await get_download_info(terabox_url, status_msg)
    
    async def download_with_resume(self, download_url: str, filename: str, status_msg, *args, **kwargs):
        """Download with resume - flexible parameter handling"""
        return await download_file(download_url, filename, status_msg)
    
    async def download_file(self, download_url: str, filename: str, status_msg, *args, **kwargs):
        """Download file method for handler compatibility"""
        return await download_file(download_url, filename, status_msg)
    
    async def cleanup_file(self, file_path: str):
        """Clean up downloaded file - ASYNC COMPATIBLE"""
        try:
            if file_path and os.path.exists(file_path):
                os.remove(file_path)
                logger.info(f"üßπ Cleaned up file: {file_path}")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è File not found for cleanup: {file_path}")
                return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not cleanup file {file_path}: {e}")
            return False

# Create global instance for backward compatibility
downloader = TeraboxDownloader()
                                   
